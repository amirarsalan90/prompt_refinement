{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from typing import List\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, conlist\n",
    "from typing import List\n",
    "\n",
    "from promptrefiner import AbstractLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "load_dotenv(\"env_variables.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class LlamaCPPModel(AbstractLLM):\n",
    "    def __init__(self, base_url, api_key):\n",
    "        super().__init__()\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        \n",
    "    def predict(self, input_text, system_prompt, temperature=0.1, max_tokens = 200):\n",
    "        response = self.client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # Update this as per your needs\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "        llm_response = response.choices[0].message.content\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "llamamodel = LlamaCPPModel(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PromptsClass:\n",
    "    \"\"\"Class for keeping track of an item in inventory.\"\"\"\n",
    "    mistral_system_prompts: List[str]\n",
    "    target_input: List[str]\n",
    "    target_output: List[str]\n",
    "    mistral_responses: List[List[str]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "class PromptClass():\n",
    "    def __init__(self, init_system_prompt: str):\n",
    "        \n",
    "        self.target_inputs = []\n",
    "        self.target_outputs = []\n",
    "\n",
    "        if not init_system_prompt:\n",
    "            raise ValueError(\"init_system_prompt cannot be empty\")\n",
    "        self.llm_system_prompts = [init_system_prompt]\n",
    "\n",
    "        self.llm_responses = []\n",
    "    \n",
    "    def add_system_prompt(self, system_prompt: str) -> None:\n",
    "        self.llm_system_prompts.append(system_prompt)\n",
    "    \n",
    "    def get_last_sys_prompt(self) -> str:\n",
    "        return self.llm_system_prompts[-1]\n",
    "    \n",
    "    def add_evaluation_example(self, input: str, output :str) -> None:\n",
    "        self.target_inputs.append(input)\n",
    "        self.target_outputs.append(output)\n",
    "    \n",
    "    def add_llm_response(self, response: List[str]) -> None:\n",
    "        self.llm_responses.append(response)\n",
    "    \n",
    "    def generate_and_add_responses(self, llm_model) -> None:\n",
    "        temp_responses = []\n",
    "        sys_prompt = self.get_last_sys_prompt()\n",
    "        for input_text in self.target_inputs:\n",
    "            output_from_llm = llm_model.predict(input_text=input_text, system_prompt=sys_prompt)\n",
    "            temp_responses.append(output_from_llm)\n",
    "        self.add_llm_response(temp_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "input_evaluation_1 = \"She said: 'today was supposed to be a day of celebration and joy in Kansas, instead it is another day where America has experience senselense gun violence' in response to what happened in Kansas near coca-cola branch\"\n",
    "\n",
    "output_evaluation_1 = [\"Gun violence\", \"Coca-cola\", \"Kansas city\"]\n",
    "\n",
    "input_evaluation_2 = \"I would say the best place to go for your honeymoon is Paris, but some say it's overrated\"\n",
    "\n",
    "output_evaluation_2 = [\"Paris\", \"Honeymoon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "init_sys_prompt = \"\"\"You are an AI designed to find a LIMITED list of GENERAL concepts associated with a given piece of text. \"\"\"\n",
    "promptTracker = PromptClass(init_system_prompt = init_sys_prompt)\n",
    "promptTracker.add_evaluation_example(input_evaluation_1, str(output_evaluation_1))\n",
    "promptTracker.add_evaluation_example(input_evaluation_2, str(output_evaluation_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You are an AI designed to find a LIMITED list of GENERAL concepts associated with a given piece of text. ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "promptTracker.llm_system_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating initial mistral input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She said: 'today was supposed to be a day of celebration and joy in Kansas, instead it is another day where America has experience senselense gun violence' in response to what happened in Kansas near coca-cola branch\n",
      "I would say the best place to go for your honeymoon is Paris, but some say it's overrated\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "for i in range(len(promptTracker.target_inputs)):\n",
    "    print(promptTracker.target_inputs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "promptTracker.llm_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTracker.generate_and_add_responses(llamamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI GPT4 for refinement of system prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "openai_sys_prompt = \"\"\"You are an AI assistant who is expert in creating promtps for LLMs. you job is to modify and enhance a prompt for a 7b mistral instruct model. The mistral model is supposed to receive an input text, and return a list of strings, entities, brand names, etc in that input text. The prompt to the mistral model can include some examples that lead the model's behavior.\n",
    "\n",
    "A number of experiments have been done on different system prompts for mistral and the output. Those experiments which include tested system prompt, tested INPUTs TO MISTRAL, and the resulting outputs from Mistral are provided to you. Your job is to observe the experiments, and come up with a better system prompt for Mistral to achieve the expected output. you can provide some examples, or remove some examples in your suggested system prompt. Remember that total number of examples should be limited to 3, because it adds extra computation and we can't afford it. Note that the examples given in the system prompt of mistral should be enclosed by ### ###. Pay attention to the fact that, you are NOT allowed to use EVALUATION INPUT TO MISTRAL texts in your examples for your suggested mistral system prompt.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Paris\\n2. Honeymoon\\n3. Travel\\n4. Overrated\\n5. Tourist destination\\n6. Romance\\n7. Europe\\n8. City break\\n9. Culture\\n10. Sightseeing\\n11. Travel experience\\n12. Tourism\\n13. Holiday\\n14. European city\\n15. Landmark\\n16. Attraction\\n17. Eiffel Tower\\n18. Louvre Museum\\n19. Notre-Dame Cathedral\\n20. Seine River\\n21. French cuisine\\n22. Parisian lifestyle\\n23. Romantic getaway\\n24. Travel advice\\n25. Personal opinion\\n26. Tourist opinion\\n27. European travel\\n28. Parisian landmarks\\n29. Parisian attractions\\n30. Parisian culture\\n31. Parisian cuisine\\n3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptTracker.llm_responses[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_openai_user_prompt(prompttracker):\n",
    "    total_prompt = \"\"\n",
    "\n",
    "    for i in range(len(prompttracker.llm_responses)):\n",
    "        total_prompt += f\"\"\"\\n\\n\\n\\n\n",
    "Experiment {i}\n",
    "Mistral System Prompt:\n",
    "{prompttracker.llm_system_prompts[i]}\n",
    "\"\"\"\n",
    "        for j in range(len(prompttracker.target_inputs)):\n",
    "            total_prompt += f\"\"\"\\n\\n\n",
    "\n",
    "EVALUATION INPUT {j} TO Local LLM:\n",
    "{prompttracker.target_inputs[j]}\n",
    "EVALUATION OUTPUT {j} FROM Local LLM:\n",
    "{prompttracker.llm_responses[i][j]}\n",
    "what was EXPECTED to be EVALUATION OUTPUT {j} from Local LLM:\n",
    "{str(prompttracker.target_outputs[j])} \\n\\n\n",
    "\"\"\"\n",
    "        \n",
    "    return total_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSystemPrompt(BaseModel):\n",
    "    Enhanced_System_Prompt: str\n",
    "\n",
    "\n",
    "def request_to_openai(prompttracker):\n",
    "    openai_user_prompt = create_openai_user_prompt(prompttracker=prompttracker)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        temperature = 0.1,\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": openai_sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": openai_user_prompt},\n",
    "            ],\n",
    "        functions=[\n",
    "            {\n",
    "            \"name\": \"Enhanced_System_Prompt\",\n",
    "            \"description\": \"Enhanced System Prompt for Local LLM\",\n",
    "            \"parameters\": EnhancedSystemPrompt.model_json_schema()\n",
    "            }\n",
    "        ],\n",
    "        function_call={\"name\": \"Enhanced_System_Prompt\"}\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.function_call.arguments)['Enhanced_System_Prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_system_prompt_with_gpt4(number_of_iterations):\n",
    "    for i in range(number_of_iterations):\n",
    "        openai_suggestion = request_to_openai(prompttracker=promptTracker)\n",
    "        promptTracker.add_system_prompt(openai_suggestion)\n",
    "        \n",
    "\n",
    "        promptTracker.generate_and_add_responses(llamamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_system_prompt_with_gpt4(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"['Gun violence', 'Coca-cola', 'Kansas city']\", \"['Paris', 'Honeymoon']\"]\n"
     ]
    }
   ],
   "source": [
    "print(promptTracker.target_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Gun Violence\\n2. Celebration\\n3. Joy\\n4. Kansas\\n5. America\\n6. Coca-Cola Branch\\n7. Senseless Violence\\n8. Shooting Incident\\n9. Safety Concerns\\n10. Community Event\\n11. Tragedy\\n12. Public Space\\n13. Security\\n14. Violence Prevention\\n15. Emergency Response\\n16. Law Enforcement\\n17. Public Awareness\\n18. Gun Control\\n19. Mourning\\n20. Community Support\\n', '1. Paris\\n2. Honeymoon\\n3. Travel\\n4. Overrated\\n5. Tourist destination\\n6. Romance\\n7. Europe\\n8. City break\\n9. Culture\\n10. Sightseeing\\n11. Travel experience\\n12. Tourism\\n13. Holiday\\n14. European city\\n15. Landmark\\n16. Attraction\\n17. Eiffel Tower\\n18. Louvre Museum\\n19. Notre-Dame Cathedral\\n20. Seine River\\n21. French cuisine\\n22. Parisian lifestyle\\n23. Romantic getaway\\n24. Travel advice\\n25. Personal opinion\\n26. Tourist opinion\\n27. European travel\\n28. Parisian landmarks\\n29. Parisian attractions\\n30. Parisian culture\\n31. Parisian cuisine\\n3']\n",
      "[\"['Kansas', 'America'] \\n\", \"['Paris'] \\n\"]\n",
      "[\"['Kansas', 'America', 'senselense', 'gun violence'] \\n\", \"['Paris'] \\n\"]\n",
      "[\"Output: ['America', 'gun violence'] \\n\", \"['Paris'] \\n\"]\n"
     ]
    }
   ],
   "source": [
    "for i in promptTracker.llm_responses:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Output: ['America', 'gun violence'] \\n\", \"['Paris'] \\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(promptTracker.llm_responses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an AI designed to extract and list a LIMITED number of SPECIFIC entities, brand names, locations, or general concepts directly mentioned or clearly implied in a given piece of text. Your task is to identify and list the explicit entities, brands, locations, or general concepts without inferring or expanding on the concepts. Focus on the most relevant and directly mentioned items in the text.\\n\\n### Example 1: \\nInput: 'The new iPhone 12 is now available at all Apple stores.' \\nOutput: ['iPhone 12', 'Apple'] \\n\\n### Example 2: \\nInput: 'The annual marathon in Boston was canceled due to unexpected weather conditions.' \\nOutput: ['Boston', 'marathon'] \\n\\n### Example 3: \\nInput: 'A tragic incident of gun violence occurred near the Coca-Cola branch in Kansas City.' \\nOutput: ['gun violence', 'Coca-Cola', 'Kansas City'] \\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptTracker.get_last_sys_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sglang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
