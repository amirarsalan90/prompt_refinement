{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(\"env_variables.env\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptrefiner import AbstractLLM, PromptTrackerClass, OpenaiCommunicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaCPPModel(AbstractLLM):\n",
    "    \"\"\"\n",
    "    A class if you are using llama-cpp-python as your local model. If you want to use a huggingface as your local model, you can use the example in readme file.\n",
    "    For other local models, follow the instructions in readme file on how to create your AbstractLLM subclass.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_url, api_key, temperature=0.1, max_tokens=200):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        \n",
    "    def predict(self, input_text, system_prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "        model=\"mistral\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ],\n",
    "        temperature=self.temperature,\n",
    "        max_tokens=self.max_tokens,\n",
    "    )\n",
    "        llm_response = response.choices[0].message.content\n",
    "        return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamamodel = LlamaCPPModel(base_url=\"http://localhost:8000/v1\", api_key=\"sk-xxx\", temperature=0.1, max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "input_evaluation_1 = \"\"\"In an era where the digital expanse collided with the realm of human creativity, two figures stood at the forefront. Ada, with her prophetic vision, laid the groundwork for computational logic, while Banksy, shrouded in anonymity, painted the streets with social commentary. Amidst this, the debate over AI ethics, spearheaded by figures like Bostrom, questioned the trajectory of our technological companions. This period, marked by innovation and introspection, challenges us to ponder the relationship between creation and creator.\"\"\"\n",
    "\n",
    "input_evaluation_2 = \"\"\"As the digital revolution accelerated, two contrasting visions emerged. Musk's endeavors to colonize Mars signified humanity's boundless ambition, while Hawking's warnings about AI highlighted the existential risks of unbridled technological advancement. Amid these towering ambitions and cautions, the resurgence of environmentalism, led by figures like Thunberg, reminded us of our responsibilities to Earth. This dichotomy between reaching for the stars and preserving our home planet defines the modern dilemma.\"\"\"\n",
    "\n",
    "output_evaluation_1 = \"\"\"\n",
    "[\"Ada Lovelace: Computational logic\", \"Banksy: Social commentary through art\", \"Nick Bostrom: AI ethics\", \"Digital era: Innovation and introspection\", \"AI ethics: The debate over the moral implications of artificial intelligence\"]\n",
    "\"\"\"\n",
    "\n",
    "output_evaluation_2 = \"\"\"\n",
    "[\"Elon Musk: Colonization of Mars\", \"Stephen Hawking: Warnings about AI\", \"Greta Thunberg: Environmentalism\", \"Digital revolution: Technological advancement and existential risks\", \"Modern dilemma: Balancing ambition with environmental responsibility\"]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_evaluations = [input_evaluation_1, input_evaluation_2]\n",
    "output_evaluations = [output_evaluation_1, output_evaluation_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "init_sys_prompt = \"\"\"You are an AI that receives an input text. Your task is to output a pythoning string where every strings is the name of a person with what they are associated with\"\"\"\n",
    "promptTracker = PromptTrackerClass(init_system_prompt = init_sys_prompt)\n",
    "promptTracker.add_evaluation_examples(input_evaluations, output_evaluations)\n",
    "#promptTracker.add_evaluation_examples(input_evaluation_2, output_evaluation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTracker.run_initial_prompt(llm_model=llamamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(promptTracker.llm_responses[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(promptTracker.llm_responses[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_communicator = OpenaiCommunicator(client=client, openai_model_code=\"gpt-4-0125-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_communicator.refine_system_prompt(prompttracker=promptTracker, llm_model=llamamodel, number_of_iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_system_prompts[0], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_responses[0][0], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_responses[0][1], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_system_prompts[1], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_responses[1][0], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textwrap.fill(promptTracker.llm_responses[1][1], width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refiner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
